{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalCode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LakshmiHegde/project/blob/master/FinalCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR9tD4fjzlxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "88421c96-6985-48f8-e1f2-afa4cb235299"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJskgRjP0zwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops.math_ops import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSMiJyf_01cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Vc2cl507-l",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaRk-TlJ0nof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7d65767e-c6ab-48c5-a16f-e34df968c5e8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8aAnQ821AV7",
        "colab_type": "text"
      },
      "source": [
        "# **Reading Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QPMyW0Vzuia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv(root_dir+\"Dataset/Reviews.csv\",nrows=100000)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "696wZWdr1SXz",
        "colab_type": "text"
      },
      "source": [
        "# **Drop duplicates and NA values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-upMW8JR0hs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop_duplicates(subset=['Text'],inplace=True)#dropping duplicates\n",
        "data.dropna(axis=0,inplace=True)#dropping na"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qgg4eXy1agX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "cfeb39b5-261b-46fc-8421-b57e711a254b"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 88421 entries, 0 to 99999\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count  Dtype \n",
            "---  ------                  --------------  ----- \n",
            " 0   Id                      88421 non-null  int64 \n",
            " 1   ProductId               88421 non-null  object\n",
            " 2   UserId                  88421 non-null  object\n",
            " 3   ProfileName             88421 non-null  object\n",
            " 4   HelpfulnessNumerator    88421 non-null  int64 \n",
            " 5   HelpfulnessDenominator  88421 non-null  int64 \n",
            " 6   Score                   88421 non-null  int64 \n",
            " 7   Time                    88421 non-null  int64 \n",
            " 8   Summary                 88421 non-null  object\n",
            " 9   Text                    88421 non-null  object\n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 7.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enua_vdv1jCB",
        "colab_type": "text"
      },
      "source": [
        "Here is the dictionary that we will use for expanding the contractions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fplscSSa1cZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRpV2K3Y1pDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:                                                 #removing short word\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMRcz8Fn2Zx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#call the function\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t,0))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAEHxHfZ2r1C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "0f2bbd2c-6276-47be-eb36-322933258bb3"
      },
      "source": [
        "cleaned_text[:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
              " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\n",
              " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
              " 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\n",
              " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wquTCvgb2yaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#call the function\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGw5XEWd20Zs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "132ef630-af27-4ace-8fbc-29c99eb76a68"
      },
      "source": [
        "cleaned_summary[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good quality dog food',\n",
              " 'not as advertised',\n",
              " 'delight says it all',\n",
              " 'cough medicine',\n",
              " 'great taffy',\n",
              " 'nice taffy',\n",
              " 'great just as good as the expensive brands',\n",
              " 'wonderful tasty taffy',\n",
              " 'yay barley',\n",
              " 'healthy dog food']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isZoRpQu22vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF4DnhF_2_mM",
        "colab_type": "text"
      },
      "source": [
        "# **Drop empty rows**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXv_-lto265B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pyAq0-Z2-23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "4dd559ea-3bb7-428a-c38f-a134025591b6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in data['cleaned_text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in data['cleaned_summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df5RU5Z3n8fcn/oprYgA1HQQzkAkmR2VEYZXZODOdGBFJNpg9iYG4AQ1HkqNm9CybCWayh4zGGbIbzciMa0IiI2RU9GiMbILBlljHeHZQQIkI6tASXJuDkAhKGqMJ5rt/3Kfipbqqu5qurqouPq9z6lTV9z731n363Opv3ec+93kUEZiZ2aHtbY3eATMzazwnAzMzczIwMzMnAzMzw8nAzMxwMjAzM5wMzMwMJwMza3KStkn6aLNsp1U5GVjVJB3e6H0ws8HhZFBnkr4iabuk30h6TtK5km6T9I1cmXZJXbn32yR9WdJTkvZJulVSm6QH0nYekjQ8lR0jKSRdKulFSXskfVHSf0zrvyLpn3Pb/lNJP5P0sqRfS7pd0rCSz/6KpKeAfWk/7i2p0yJJNw3qH84OSZJ+ALwX+D+SuiX9jaTJkv5vOpZ/Iak9lf1P6Rg+Kb0/PR3/Hyy3nYZVqllFhB91egAfAF4ETkzvxwB/CtwGfCNXrh3oyr3fBqwB2oBRwC7gCeAM4O3Az4AFuW0G8J20bArwOvAj4N259f8qlX8/cB5wFHAC8AjwjyWfvQE4CTgaGAnsA4al5Yen7U1s9N/Xj9Z8pGPwo+n1KOBlYBrZj9nz0vsT0vLr0/fhaGAjcGW57fjR8+Ezg/p6k+yf7imSjoiIbRHxfJXr/lNE7IyI7cDPgcci4smIeB24jywx5F0XEa9HxINk/7zvjIhdufXPAIiIzojoiIg3IuJXwI3AX5Vsa1FEvBgRv42IHWQJ49Np2VTg1xGxvl9/CbOD81+BlRGxMiL+EBEdwDqy5ADwdeBdwOPAduDmhuzlEORkUEcR0QlcTXbA7pK0XNKJVa6+M/f6t2Xev+NgyqfmpuWp6Wov8K/A8SXberHk/VKyLyXp+QdV1sFsoP4E+HRqInpF0ivAOWRnrETE78nOtE8Dboh0SmB9czKos4i4IyLOITuoA/gm2S/3/5Ar9p467tLfp/0YHxHHkv1zV0mZ0i/Uj4A/k3Qa8HHg9kHfSzuU5Y+/F4EfRMSw3OOYiFgIIGkUsAD4F+AGSUdV2I6VcDKoI0kfkPSRdIC+TvYL/Q9kbfLTJI2Q9B6ys4d6eSfQDbyavkhf7muF1DR1D3AH8HhE/L/B3UU7xO0E3pde/yvwnyWdL+kwSW9PHS5GSxLZWcGtwBxgB3Bdhe1YCSeD+joKWAj8GniJ7ILuNWTNLL8gu8D1IHBXHffp74AzgVeBnwA/rHK9pcB43ERkg+8fgK+lJqHPANOBrwK/IjtT+DLZ/7K/JvtO/Y/UPHQpcKmkvyjdjqT/Xuc6ND25Sc0OhqT3As8C74mIvY3eHzMbGJ8ZWL9Jehvw34DlTgRmrcF3lFq/SDqGrO31BbJupWbWAtxMZGZmfTcTSTpJ0sOSNkvaJOmqFB8hqUPSlvRcHA5BaXiCzjT8wZm5bc1O5bdImp2LT5S0Ma2zKPUKMDOzOunzzEDSSGBkRDwh6Z3AeuBC4BJgd0QslDQfGB4RX5E0DfgS2R2BZwM3RcTZkkaQ3Sk4iay/73qyIQz2SHqcrCfAY8BKsjteH+htv44//vgYM2YM+/bt45hjjjnoP0AzcB0aY/369b+OiBMavR/VKh7zpYbi374artfgqHjc93f8CuB+svFAniNLEpDd/fdcev1dYGau/HNp+Uzgu7n4d1NsJPBsLn5AuUqPiRMnRkTEww8/HEOd69AYwLpogjFhqn0Uj/lSQ/FvXw3Xa3BUOu77dQFZ0hiyMW0eA9oiG6cGsj7zben1KA4cvqArxXqLd5WJl/v8ucBcgLa2NgqFAt3d3RQKhf5Uo+m4DmbWaFUnA0nvAO4Fro6Ivflm/YgISYN+JToiFgOLASZNmhTt7e0UCgXa29sH+6MHletgZo1W1X0Gko4gSwS3R0TxDtWd6XpC8brCrhTfTjbccdHoFOstPrpM3MzM6qSa3kQiG+vjmYi4MbdoBVDsETSb7FpCMT4r9SqaDLyampNWAVMkDU89j6YAq9KyvWnCCgGzctsyM7M6qKaZ6EPA54CNkjak2FfJxti5W9IcshuQLkrLVpL1JOoEXiMbH4SI2C3pOmBtKndtROxOry8nG2DqaOCB9DAzszrpMxlExKP0HNK46Nwy5QO4osK2lgBLysTXkY0/bmZmDeCxiczMzMnAzMycDMzMjENk1NIx839ywPttCz/WoD0xGxw+xm2gfGZgZmZOBmZm5mRgZmY4GZiZGU4GZmaGk4GZmeFkYFaWpGGS7pH0rKRnJP25p3q1VuZkYFbeTcBPI+KDwOnAM8B8YHVEjANWp/cAFwDj0mMucAtk84QDC8imfz0LWFBMIKnMZbn1ptahTmYVORmYlZD0LuAvyYZuJyJ+FxGvANOBpanYUrK5wEnxZWlWwTXAsDTHx/lAR0Tsjog9QAcwNS07NiLWpIEdl+W2ZdYQh8QdyGb9NBb4FfAvkk4H1gNX0SRTvZbq7u5m3vg3D4i1whSkrTqVarPWy8nArKfDgTOBL0XEY5Ju4q0mIaCxU72WKhQK3PDovgNi2y7uWW6oadWpVJu1Xm4mMuupC+iKiMfS+3vIkoOnerWW5WRgViIiXgJelPSBFDoX2IynerUW5mYis/K+BNwu6UhgK9n0rW/DU71ai+ozGUhaAnwc2BURp6XYXUDxV9Mw4JWImCBpDFkXvOfSsjUR8cW0zkTeOvhXAleldtcRwF3AGGAbcFHqeWHWMBGxAZhUZpGnerWWVE0z0W2U9IGOiM9ExISImADcC/wwt/j54rJiIkgq9auu1HfbzMzqpM9kEBGPALvLLUvtnRcBd/a2jT76VVfqu21mZnUy0GsGfwHsjIgtudhYSU8Ce4GvRcTP6b1fdaW+2z2U63NdTZ/deeP3H/C+2fr4Nmu/4/5ohTqYHcoGmgxmcuBZwQ7gvRHxcrpG8CNJp1a7sb76bpfrc11Nn91LSqcEbLI+2M3a77g/WqEOZoeyg04Gkg4H/gswsRiLiDeAN9Lr9ZKeB06m937VOyWNjIgdJX23zcysTgZyn8FHgWcj4o/NP5JOkHRYev0+sgvFW/voV12p77aZmdVJn8lA0p3AvwEfkNSV+lgDzKDnheO/BJ6StIHsrs0vlvSr/j5ZX+zneatf9ULgPElbyBLMwgHUx8zMDkKfzUQRMbNC/JIysXvJupqWK1+2X3VEvEyZvttmZlY/Ho7CzMycDMzMzMnAzMw4RAeqG1Ny3wHAtoUfa8CemJk1B58ZmJmZk4GZmTkZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGU4GZmaGk4GZmeFkYGZmOBmYmRlOBmZmhpOBWVmStknaKGmDpHUpNkJSh6Qt6Xl4ikvSIkmdkp6SdGZuO7NT+S2SZufiE9P2O9O6qn8tzd7iZGBW2YcjYkJETErv5wOrI2IcsDq9B7iAbL7vccBc4BbIkgewADgbOAtYUEwgqcxlufWmDn51zCqrZg7kJZJ2SXo6F/u6pO3pV9MGSdNyy65Jv3aek3R+Lj41xTolzc/Fx0p6LMXvknRkLStoVkPTgaXp9VLgwlx8WWTWAMMkjQTOBzoiYndE7AE6gKlp2bERsSYiAliW25ZZQ1Qzn8FtwD+THbB5346Ib+UDkk4BZgCnAicCD0k6OS2+GTgP6ALWSloREZuBb6ZtLZf0HWAO6ZeVWQMF8KCkAL4bEYuBtojYkZa/BLSl16OAF3PrdqVYb/GuMvEeJM0lO9ugra2NQqHQo0x3dzfzxr95QKxcuaGmu7u7JepRqlnr1WcyiIhHJI2pcnvTgeUR8QbwS0mdZKfHAJ0RsRVA0nJguqRngI8An01llgJfx8nAGu+ciNgu6d1Ah6Rn8wsjIlKiGFQpCS0GmDRpUrS3t/coUygUuOHRfQfEtl3cs9xQUygUKFffoa5Z6zWQmc6ulDQLWAfMS6fBo4A1uTL5Xzylv5DOBo4DXomI/WXK91DuV1I1WXbe+P29LofG/pJq1l8K/dEKdciLiO3peZek+8h+1OyUNDIidqSmnl2p+HbgpNzqo1NsO9BeEi+k+Ogy5c0a5mCTwS3AdWSn0tcBNwCfr9VOVVLuV1I1WfaSMtNclmrkL6lm/aXQH61QhyJJxwBvi4jfpNdTgGuBFcBsYGF6vj+tsoLsx9Fysh85r6aEsQr4+9xF4ynANRGxW9JeSZOBx4BZwD/Vq35m5RxUMoiIncXXkr4H/Di9rfQLiQrxl8kuth2ezg78C8maQRtwX+rteThwR0T8VNJa4G5Jc4AXgItS+ZXANKATeA24FCD9078OWJvKXRsRu9Pry8muxx0NPJAeZg1zUMmgeKqc3n4SKPY0WgHcIelGsgvI44DHAQHjJI0l+2c/A/hsand9GPgUsJwDf22ZNUS6tnV6mfjLwLll4gFcUWFbS4AlZeLrgNMGvLNmNdJnMpB0J1m75/GSusj6TbdLmkDWTLQN+AJARGySdDewGdgPXBERb6btXAmsAg4DlkTEpvQRXwGWS/oG8CRwa81qZ2ZmVammN9HMMuGK/7Aj4nrg+jLxlWSn06XxrbzV48jMzBrAdyCbmZmTgZmZORmYmRlOBmZmhpOBmZnhZGBmZgxsbKKWMqZkyIptCz/WoD0xM6s/nxmYmZmTgZmZORmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGVUkA0lLJO2S9HQu9r8kPSvpKUn3SRqW4mMk/VbShvT4Tm6diZI2SuqUtEiSUnyEpA5JW9Lz8MGoqJmZVVbNmcFtwNSSWAdwWkT8GfDvwDW5Zc9HxIT0+GIufgtwGTAuPYrbnA+sjohxwOr03szM6qjPZBARjwC7S2IPRsT+9HYNMLq3bUgaCRwbEWsiIoBlwIVp8XRgaXq9NBc3M7M6qcUQ1p8H7sq9HyvpSWAv8LWI+DkwCujKlelKMYC2iNiRXr8EtFX6IElzgbkAbW1tFAoFuru7KRQKve7gvPH7e11eTl/brKVq6tDsWqEOpSQdBqwDtkfExyWNBZYDxwHrgc9FxO8kHUX2A2ci8DLwmYjYlrZxDTAHeBP464hYleJTgZuAw4DvR8TCulbOrMSAkoGkvwX2A7en0A7gvRHxsqSJwI8knVrt9iIiJEUvyxcDiwEmTZoU7e3tFAoF2tvbe93uJSVzFVRj28W9b7OWqqlDs2uFOpRxFfAMcGx6/03g2xGxPF0Pm0PW/DkH2BMR75c0I5X7jKRTgBnAqcCJwEOSTk7buhk4j+yH0VpJKyJic70qZlbqoHsTSboE+DhwcWr6ISLeiIiX0+v1wPPAycB2DmxKGp1iADtTM1KxOWnXwe6TWa1IGg18DPh+ei/gI8A9qUi+STPf1HkPcG4qPx1Ynr4XvwQ6gbPSozMitkbE78jONqYPfq3MKjuoZJBOcf8G+EREvJaLn5BOrZH0PrILxVtTM9BeSZPTl2QWcH9abQUwO72enYubNdI/kh3jf0jvjwNeyV0ryzd1jgJeBEjLX03l/xgvWadS3Kxh+mwmknQn0A4cL6kLWEDWe+gooCP1EF2Teg79JXCtpN+TfYm+GBHFi8+Xk/VMOhp4ID0AFgJ3S5oDvABcVJOamR0kSR8HdkXEekntDd6XHtfJSnV3dzNv/JsHxFrh+k0rXoeC5q1Xn8kgImaWCd9aoey9wL0Vlq0DTisTfxk4t6/9MKujDwGfkDQNeDvZNYObgGGSDk+//vNNnduBk4AuSYcD7yK7kFyMF+XXqRQ/QLnrZKUKhQI3PLrvgFg9r3kNlha9DtW09fIdyGYlIuKaiBgdEWPILgD/LCIuBh4GPpWK5Zs0802dn0rlI8VnSDoq9UQaBzwOrAXGSRor6cj0GSvqUDWzimrRtdTsUPEVYLmkbwBP8tYZ8q3ADyR1kt2TMwMgIjZJuhvYTNbr7oqIeBNA0pXAKrKupUsiYlNda2JWwsnArBcRUQAK6fVWsp5ApWVeBz5dYf3rgevLxFcCK2u4q2YD4mYiMzNzMjAzMycDMzPDycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzM5wMzMwMJwMzM8PJwMzMcDIwMzOcDMzMDCcDMzOjymQgaYmkXZKezsVGSOqQtCU9D09xSVokqVPSU5LOzK0zO5XfIml2Lj5R0sa0ziKliZXNzKw+qj0zuA2YWhKbD6yOiHHA6vQe4AKy6f3GkU3kfQtkyQNYAJxNNkHIgmICSWUuy61X+ll1N2b+Tw54mJm1sqqSQUQ8QjadX950YGl6vRS4MBdfFpk1ZJOIjwTOBzoiYndE7AE6gKlp2bERsSbNG7ssty0zM6uDgUx72RYRO9Lrl4C29HoU8GKuXFeK9RbvKhPvQdJcsrMN2traKBQKdHd3UygUet3ReeP3V1Gd3vX1GQNRTR2aXSvUwexQVpM5kCMiJEUtttXH5ywGFgNMmjQp2tvbKRQKtLe397reJTVo5tl2ce+fMRDV1KHZtUIdzA5lA+lNtDM18ZCed6X4duCkXLnRKdZbfHSZuJmZ1clAksEKoNgjaDZwfy4+K/Uqmgy8mpqTVgFTJA1PF46nAKvSsr2SJqdeRLNy2zIzszqoqplI0p1AO3C8pC6yXkELgbslzQFeAC5KxVcC04BO4DXgUoCI2C3pOmBtKndtRBQvSl9O1mPpaOCB9DAzszqpKhlExMwKi84tUzaAKypsZwmwpEx8HXBaNftiZma15zuQzcqQ9HZJj0v6haRNkv4uxcdKeizdIHmXpCNT/Kj0vjMtH5Pb1jUp/pyk83PxqSnWKWl+6T6Y1ZOTgVl5bwAfiYjTgQlk98RMBr4JfDsi3g/sAeak8nOAPSn+7VQOSacAM4BTyW6m/N+SDpN0GHAz2U2apwAzU1mzhnAyMCsj3TTZnd4ekR4BfAS4J8VLb7Ys3oR5D3Bu6hAxHVgeEW9ExC/JrqWdlR6dEbE1In4HLE9lzRqiJvcZmLWi9Ot9PfB+sl/xzwOvRETxLsb8DZJ/vKkyIvZLehU4LsXX5DabX6f0Jsyzy+xDjxstS3V3dzNv/JsHxFrhBsBWvZGxWevlZGBWQUS8CUyQNAy4D/hgA/ahx42WpQqFAjc8uu+A2GDeJFkvrXojY7PWy81EZn2IiFeAh4E/Jxtrq/gjKn+D5B9vqkzL3wW8TP9vwjRriJY7M/AIo1YLkk4Afh8Rr0g6GjiP7KLww8CnyNr4S2+2nA38W1r+szRMywrgDkk3AieSjcr7OCBgnKSxZElgBvDZetXPrFTLJQOzGhkJLE3XDd4G3B0RP5a0GVgu6RvAk8CtqfytwA8kdZKN8DsDICI2Sbob2AzsB65IzU9IupLszvzDgCURsal+1TM7kJOBWRkR8RRwRpn4VrKeQKXx14FPV9jW9cD1ZeIrye7YN2s4XzMwMzMnAzMzczIwMzOcDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnAzMzYwDJQNIHJG3IPfZKulrS1yVtz8Wn5dbxjE9mZk3ooIejiIjnyGaAKo77vp1smN9LyWaC+la+fMmMTycCD0k6OS2+mWwgsC5graQVEbH5YPfNzMz6p1ZjE50LPB8RL2STO5X1xxmfgF+mAb2KY7x0pjFfkFSc8cnJwMysTmqVDGYAd+beXylpFrAOmBcRexjgjE9Qftan0lmD5o3fX27VARvMmYmadeaj/miFOpgdygacDCQdCXwCuCaFbgGuI5sv9jrgBuDzA/0cKD/rU+msQZcM0nwGgzlzVLPOfNQfrVAHs0NZLc4MLgCeiIidAMVnAEnfA36c3vY2s5NnfDIza6BadC2dSa6JSNLI3LJPAk+n1yuAGZKOSrM7FWd8Wkua8SmdZcxIZc3MrE4GdGYg6RiyXkBfyIX/p6QJZM1E24rLPOOTmVnzGlAyiIh9wHElsc/1Un7IzvhUbm7lbQs/1oA9MTOrPd+BbGZmTgZmZuZkYGZmOBmYmRlOBmZmhpOBmZnhZGDWg6STJD0sabOkTZKuSvERkjokbUnPw1NckhalIdifknRmbluzU/ktkmbn4hMlbUzrLFIvIzya1YOTgVlP+8kGWDwFmAxckYZgnw+sjohxwOr0HrIhWcalx1yy8bmQNAJYQDbw4lnAgmICSWUuy603tQ71MqvIycCsRETsiIgn0uvfAM+QjbA7HViaii0FLkyvpwPLIrMGGJaGZTkf6IiI3Wnk3g5galp2bESsiYgAluW2ZdYQtRrC2qwlSRoDnAE8BrRFxI606CWgLb0eRc9h2Ef1Ee8qEy/3+T2GbS/V3d3NvPFvHhBrheHEW3VY9Gatl5OBWQWS3gHcC1wdEXvzzfoREZJisPeh3LDtpQqFAjc8uu+A2GAOuV4vrToserPWy81EZmVIOoIsEdweET9M4Z3FUXnT864UrzQ8e2/x0WXiZg3jZGBWIvXsuRV4JiJuzC1aARR7BM0G7s/FZ6VeRZOBV1Nz0ipgiqTh6cLxFGBVWrZX0uT0WbNy2zJrCDcTmfX0IeBzwEZJG1Lsq8BC4G5Jc4AXgIvSspXANKATeA24FCAidku6jmzODoBrI2J3en05cBtwNPBAepg1jJOBWYmIeBSo1O//3DLlA7iiwraWAEvKxNcBpw1gN81qys1EZmbmMwOzVlQ6GZMnYrK++MzAzMwGngwkbUtjrGyQtC7FajaGi5mZDb5anRl8OCImRMSk9L6WY7iYmdkgG6xmopqM4TJI+2ZmZiVqcQE5gAfTrfnfTbfP12oMlwOUG6eldJyPeeP316BK1anV+CLNOlZJf7RCHcwOZbVIBudExHZJ7wY6JD2bX1jLMVzKjdNSOs7HJSW9KAZTrcZ/adaxSvqjFepgdigbcDNRRGxPz7uA+8ja/Gs1houZmdXBgJKBpGMkvbP4mmzslaep0RguA9k3MzOr3kCbidqA+9LQvocDd0TETyWtpXZjuJiZ2SAbUDKIiK3A6WXiL1OjMVzMzGzweTiKAfAt/2bWKjwchZmZORmYmZmTgZmZ4WRgZmY4GZiZGU4GZmaGk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZnhZGBWlqQlknZJejoXGyGpQ9KW9Dw8xSVpkaROSU9JOjO3zuxUfouk2bn4REkb0zqLlIb+NWsUJwOz8m6j5zzc84HVETEOWJ3eA1wAjEuPucAtkCUPYAFwNtmkTwuKCSSVuSy3nuf8toZyMjArIyIeAUrn1JgOLE2vlwIX5uLLIrMGGJZm+Dsf6IiI3RGxB+gApqZlx0bEmjSs+7LctswawkNYm1WvLc3MB/AS2eROAKOAF3PlulKst3hXmXgPkuaSnW3Q1tZGoVDoUaa7u5t549/sdcfLrdfsuru7h+R+96VZ6+VkUEOl8xuA5zhoVRERkqIOn7MYWAwwadKkaG9v71GmUChww6P7et3Otot7rtfsCoUC5eo71DVrvdxMZFa9namJh/S8K8W3Ayflyo1Osd7io8vEzRrmoJOBpJMkPSxps6RNkq5K8a9L2i5pQ3pMy61zTeo98Zyk83PxqSnWKWl+uc8zawIrgGKPoNnA/bn4rNSraDLwampOWgVMkTQ8XTieAqxKy/ZKmpx6Ec3KbcusIQbSTLQfmBcRT0h6J7BeUkda9u2I+Fa+sKRTgBnAqcCJwEOSTk6LbwbOI2s7XStpRURsHsC+mQ2IpDuBduB4SV1kvYIWAndLmgO8AFyUiq8EpgGdwGvApQARsVvSdcDaVO7aiChelL6crMfS0cAD6WHWMAedDNKvmx3p9W8kPUOFi2DJdGB5RLwB/FJSJ1l3O4DOiNgKIGl5KutkYA0TETMrLDq3TNkArqiwnSXAkjLxdcBpA9lHs1qqyQVkSWOAM4DHgA8BV0qaBawjO3vYQ5Yo1uRWy/egKO1xcXaFz+nRs6L0yvy88fsHXqEaqqbXQLP2LuiPVqiD2aFswMlA0juAe4GrI2KvpFuA64BIzzcAnx/o50D5nhWlV+YvKdOjp5Gq6cXRrL0L+qMV6mB2KBtQMpB0BFkiuD0ifggQETtzy78H/Di9rdSzgl7iZmZWBwPpTSTgVuCZiLgxFx+ZK/ZJoDi2ywpghqSjJI0luwX/cbKLa+MkjZV0JNlF5hUHu19mZtZ/Azkz+BDwOWCjpA0p9lVgpqQJZM1E24AvAETEJkl3k10Y3g9cERFvAki6kqwb3mHAkojYNID9MjOzfhpIb6JHgXIjLa7sZZ3rgevLxFf2tt5QVnpXsu9INrNm5DuQzczMycDMzJwMzMwMJwMzM8PJwMzMcDIwMzM8uY3ZIcETL1lffGZgZmY+M2gGG7e/esAAe/7FZmb15jMDMzNzMjAzMycDMzPDycDMzPAF5KbkkU7NrN58ZmBmZk4GZmbmZqIhwXeP2mBwc6Tl+czAzMya58xA0lTgJrJ5kL8fEQsbvEtNzb/qhj4f89ZMmiIZSDoMuBk4D+gC1kpaERGbG7tnQ4ebkoaWZjzmfQwd2poiGQBnAZ0RsRVA0nJgOuBkMADlvtz95X8Gg2ZIHPPVHEM+RlpDsySDUcCLufddwNmlhSTNBeamt92SngOOB3496HtYI/pm2XDT1qHC/pbTtHXoxZ808LMHcsyXaujfvh/HSH8NxWOqGo2uV9njvlmSQVUiYjGwOB+TtC4iJjVol2rCdbBKyh3zpVr1b+961Vez9CbaDpyUez86xcxalY95ayrNkgzWAuMkjZV0JDADWNHgfTIbTD7mrak0RTNRROyXdCWwiqyb3ZKI2FTl6r2eQg8RrsMhZoDHfKlW/du7XnWkiGj0PpiZWYM1SzORmZk1kJOBmZkN3WQgaaqk5yR1Sprf6P2phqQlknZJejoXGyGpQ9KW9Dy8kfvYF0knSXpY0mZJmyRdleJDqh6tYih+D4okbZO0UdIGSetSrOxxpMyiVM+nJJ3Z2L1/S3++173VQ9LsVH6LpNn1rseQTAa5W8mY91IAAAJKSURBVPkvAE4BZko6pbF7VZXbgKklsfnA6ogYB6xO75vZfmBeRJwCTAauSH/7oVaPIW8Ifw/yPhwRE3L97isdRxcA49JjLnBL3fe0stuo/ntdth6SRgALyG48PAtYUO8fVEMyGZC7lT8ifgcUb+VvahHxCLC7JDwdWJpeLwUurOtO9VNE7IiIJ9Lr3wDPkN1NO6Tq0SKG5PegD5WOo+nAssisAYZJGtmIHSzVz+91pXqcD3RExO6I2AN00DPBDKqhmgzK3co/qkH7MlBtEbEjvX4JaGvkzvSHpDHAGcBjDOF6DGFD/XsQwIOS1qdhN6DycTTU6trfejS8fk1xn4FlIiIkDYm+vpLeAdwLXB0ReyX9cdlQqoc11DkRsV3Su4EOSc/mF7bKcTRU6jFUzwxa6Vb+ncXT3fS8q8H70ydJR5Algtsj4ocpPOTq0QKG9PcgIran513AfWTNXpWOo6FW1/7Wo+H1G6rJoJVu5V8BFHsOzAbub+C+9EnZKcCtwDMRcWNu0ZCqR4sYst8DScdIemfxNTAFeJrKx9EKYFbqjTMZeDXXDNOM+luPVcAUScPTheMpKVY/ETEkH8A04N+B54G/bfT+VLnPdwI7gN+TtQnOAY4j622wBXgIGNHo/eyjDueQtfU+BWxIj2lDrR6t8hiK34O03+8DfpEem4r7Xuk4AkTWc+p5YCMwqdF1yNWl6u91b/UAPg90psel9a6Hh6MwM7Mh20xkZmY15GRgZmZOBmZm5mRgZmY4GZiZGU4GZmaGk4GZmQH/H5pi4keT0YYpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Woxnks3GoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ffa8ccae-0567-45e6-8aca-254b1f2c5b47"
      },
      "source": [
        "cnt=0\n",
        "for i in data['cleaned_summary']:\n",
        "    if(len(i.split())<=8):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(data['cleaned_summary']))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9424907471335922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ6qjGH63LJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_text_len=30\n",
        "max_summary_len=8\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azt8g_xi3NcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_text =np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7JX0BoS3Tfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFVW7o8o3WSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnue7YKa3bx-",
        "colab_type": "text"
      },
      "source": [
        "# **Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RNxjv843nLW",
        "colab_type": "text"
      },
      "source": [
        "Text Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_0Hvl343ZpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhsjumWc3w2t",
        "colab_type": "text"
      },
      "source": [
        "# **Rarewords and its coverage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFiKToGU3uMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d0da4da2-b18e-4eb7-fd70-2f43f48219a0"
      },
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 66.12339930151339\n",
            "Total Coverage of rare words: 2.953684513790566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHwAfuLr357W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLZKa1eO3_cp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e69f9b5b-022d-48c9-bd25-eeca4c670385"
      },
      "source": [
        "x_voc"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8440"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXU73bNy4BKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkGVdGV54DwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "374b42a4-9cd5-4c76-bad8-0d0b643c6881"
      },
      "source": [
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 78.12740675541863\n",
            "Total Coverage of rare words: 5.3921899389571895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMrZZ_Qb4G7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc  =   y_tokenizer.num_words +1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g2loFXA4KEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "687ce295-13de-4bbb-b196-baca3542cbe9"
      },
      "source": [
        "y_tokenizer.word_counts['sostok'],len(y_tr)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42453, 42453)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSnwh_B74MQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2nSay_04Q8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SbUZC0g4UEx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "a46d44b5-7b4e-49df-8fb7-4a04675f79af"
      },
      "source": [
        "from keras import backend as K \n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention input and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 30, 100)      844000      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 30, 300), (N 481200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 30, 300), (N 721200      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    198900      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 30, 300), (N 721200      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 1989)   1195389     concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 4,823,389\n",
            "Trainable params: 4,823,389\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tG9w9I_4a-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBKzM7gN4ewc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G4JcfY74gvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "a0a8c29c-8e2d-4709-9a8d-3fc79ce46b5e"
      },
      "source": [
        "\n",
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "324/324 [==============================] - 125s 385ms/step - loss: 2.8128 - val_loss: 2.5781\n",
            "Epoch 2/50\n",
            "324/324 [==============================] - 124s 382ms/step - loss: 2.5048 - val_loss: 2.4586\n",
            "Epoch 3/50\n",
            "324/324 [==============================] - 122s 378ms/step - loss: 2.3476 - val_loss: 2.3330\n",
            "Epoch 4/50\n",
            "324/324 [==============================] - 124s 383ms/step - loss: 2.2422 - val_loss: 2.2371\n",
            "Epoch 5/50\n",
            "324/324 [==============================] - 125s 386ms/step - loss: 2.1702 - val_loss: 2.1954\n",
            "Epoch 6/50\n",
            "324/324 [==============================] - 125s 386ms/step - loss: 2.1100 - val_loss: 2.1542\n",
            "Epoch 7/50\n",
            "324/324 [==============================] - 126s 390ms/step - loss: 2.0597 - val_loss: 2.1265\n",
            "Epoch 8/50\n",
            "324/324 [==============================] - 125s 387ms/step - loss: 2.0143 - val_loss: 2.1073\n",
            "Epoch 9/50\n",
            "324/324 [==============================] - 124s 384ms/step - loss: 1.9755 - val_loss: 2.0975\n",
            "Epoch 10/50\n",
            "324/324 [==============================] - 124s 381ms/step - loss: 1.9401 - val_loss: 2.0732\n",
            "Epoch 11/50\n",
            "324/324 [==============================] - 124s 383ms/step - loss: 1.9047 - val_loss: 2.0541\n",
            "Epoch 12/50\n",
            "324/324 [==============================] - 123s 381ms/step - loss: 1.8743 - val_loss: 2.0412\n",
            "Epoch 13/50\n",
            "324/324 [==============================] - 123s 381ms/step - loss: 1.8451 - val_loss: 2.0466\n",
            "Epoch 14/50\n",
            "324/324 [==============================] - 124s 383ms/step - loss: 1.8173 - val_loss: 2.0447\n",
            "Epoch 00014: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff8D-hAj4kMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1e1db551-0ba8-458e-9a9b-2451d00772a5"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G813qCi37S1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLqJAxV77VKh",
        "colab_type": "text"
      },
      "source": [
        "# **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgif4vKA7aTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaLPGEqZ7f16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb5FHwqv7o-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9mtdvQe7rgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "b5897a56-c0a7-4ac8-92fc-678f37856ea1"
      },
      "source": [
        "for i in range(0,10):\n",
        "    print(\"Review:\",seq2text(x_tr[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: gave caffeine shakes heart anxiety attack plus tastes unbelievably bad stick coffee tea soda thanks \n",
            "Original summary: hour \n",
            "Predicted summary:  green tea\n",
            "\n",
            "\n",
            "Review: got great course good belgian chocolates better \n",
            "Original summary: would like to give it stars but \n",
            "Predicted summary:  good\n",
            "\n",
            "\n",
            "Review: one best flavored coffees tried usually like flavored coffees one great serve company love \n",
            "Original summary: delicious \n",
            "Predicted summary:  great coffee\n",
            "\n",
            "\n",
            "Review: salt separate area pain makes hard regulate salt putting like salt go ahead get product \n",
            "Original summary: tastes ok packaging \n",
            "Predicted summary:  salt\n",
            "\n",
            "\n",
            "Review: really like product super easy order online delivered much cheaper buying gas station stocking good long drives \n",
            "Original summary: turkey jerky is great \n",
            "Predicted summary:  great product\n",
            "\n",
            "\n",
            "Review: best salad dressing delivered promptly quantities last vidalia onion dressing compares made oak hill farms sometimes find costco order front door want even orders cut shipping costs \n",
            "Original summary: my favorite salad dressing \n",
            "Predicted summary:  the best\n",
            "\n",
            "\n",
            "Review: think sitting around warehouse long time took long time send got tea tasted like cardboard red rasberry leaf tea know supposed taste like \n",
            "Original summary: stale \n",
            "Predicted summary:  not as good as other tea\n",
            "\n",
            "\n",
            "Review: year old cat special diet digestive problems also diabetes stopped eating usual special formula food tried different kinds catfood one liked easy digestion diabetes thank newman \n",
            "Original summary: wonderful \n",
            "Predicted summary:  great food\n",
            "\n",
            "\n",
            "Review: always perfect snack dog loves knows exactly starts ask time evening gets greenie snack thank excellent product fast delivery \n",
            "Original summary: greenies buddy treat \n",
            "Predicted summary:  great treat\n",
            "\n",
            "\n",
            "Review: dog loves tiny treats keep one car one house \n",
            "Original summary: dog loves them \n",
            "Predicted summary:  my dog loves these\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCzUYzTKFK1s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90b2c66b-5c21-4645-9ded-35597c1b5a2a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbedd0M27yE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc=root_dir+\"documents\"\n",
        "os.chdir(doc)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnoDNxl2gw0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f0ba41eb-b82f-45bb-e316-66cba667249f"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/documents'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDu2YAangxWj",
        "colab_type": "text"
      },
      "source": [
        "## **INFERENCE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IizgoTGHg0RG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "4ee744cc-57b8-4bcc-e38e-a29b05aa3ee9"
      },
      "source": [
        "!pip install PyPDF2\n",
        "!pip install docx2txt\n",
        "!pip install sys\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-cp36-none-any.whl size=61086 sha256=e961c7754563443ff429922953293436a7b100a90da809cf576b032744d7c017\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n",
            "Collecting docx2txt\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp36-none-any.whl size=3963 sha256=6f439aa235e8f40fac51d140119560185584d1723b8e568d8290545ab267fa9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sys (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for sys\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD9IygqNg9xN",
        "colab_type": "text"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnzyMOROg8-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numpy library helps in working with arrays: array creation and manipulation\n",
        "# this implementation uses array for storing the matrices generated as 2-D arrays\n",
        "# PyPDF2 is a library used for reading the PDF files\n",
        "# docx2txt is the library used for reading Word documents \n",
        "# sys library has been used for printing the size of data structures used in the program\n",
        "\n",
        "\n",
        "### 1. Importing important libraries\n",
        "\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import docx2txt\n",
        "import sys\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXD49aonhHSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# matplotlib is a library that is used to visualize the data by drawing graphs of matrix inputs\n",
        "# we will use it for drawing the matrices generated later in the program \n",
        "# %matplotlib inline is a command used to show the graphs in the jupyter notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc7GSS6ihJ65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# networkx library helps in working with graphs ...\n",
        "# and later performing the PageRank algorithm ...\n",
        "# which is the crux of this implementation to find ...\n",
        "# the importance of each sentence using their 'rank' as a metric ...\n",
        "# rank, the output of the method textrank, is a measure of importance of sentences\n",
        "# this library has been used in the cell no. ()\n",
        "\n",
        "import networkx as nx\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qflVtvIhMi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the PunktSentenceTokenizer library is being imported from the file punkt.py contained in package nltk.tokenize \n",
        "# this is used to tokenize the document into sentences\n",
        "\n",
        "# Tokenization: Tokenization is the process of demarcating and possibly classifying.. \n",
        "# sections of a string of input characters. \n",
        "# The resulting tokens are then passed on to some other form of processing. \n",
        "\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irqoaj3RhQb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TfidfTransformer and CountVectorizer libraries are being imported\n",
        "\n",
        "# CountVectorizer: In this implementation, a CountVectorizer object is being created that ..\n",
        "# will be used for creating the document-term matrix\n",
        "\n",
        "# tFidTransformer: In this implementation,TfidfTransformer is used for executing the method fit_transform()... \n",
        "# which provides the output as a document-term matrix normalized (value 0-1) according to the TF-IDF\n",
        "# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)\n",
        "# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus\n",
        "# Corpus: set of all sentences\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHWggDn3hXCU",
        "colab_type": "text"
      },
      "source": [
        "# **2. Function to read the document from user**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoOaKPP_hkmi",
        "colab_type": "text"
      },
      "source": [
        "Supported formats: .txt, .pdf, .docx \n",
        "\n",
        "Input: Takes the name of the file as input. \n",
        "\n",
        "Output: Returns a string output containing the contents of the file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC4aul6QhUDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readDoc():\n",
        "    name = input('Please input a file name: ') \n",
        "    print('You have asked for the document {}'.format(name))\n",
        "\n",
        "    # now read the type of document\n",
        "    if name.lower().endswith('.txt'):\n",
        "        choice = 1\n",
        "    elif name.lower().endswith('.pdf'):\n",
        "        choice = 2\n",
        "    elif name.lower().endswith('.docx'):\n",
        "        choice = 3\n",
        "    else:\n",
        "        choice = 4\n",
        "        # print(name)\n",
        "    #print(choice)\n",
        "    # Case 1: if it is a .txt file\n",
        "        \n",
        "    if choice == 1:\n",
        "        with open(doc+'/'+name, encoding=\"utf8\", errors='ignore') as f:\n",
        "          document = f.read()\n",
        "        f.close()\n",
        "            \n",
        "    # Case 2: if it is a .pdf file\n",
        "    elif choice == 2:\n",
        "        pdfFileObj = open(name, 'rb')\n",
        "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "        pageObj = pdfReader.getPage(0)\n",
        "        document = pageObj.extractText()\n",
        "        pdfFileObj.close()\n",
        "        \n",
        "    elif choice == 3:\n",
        "        document=docx2txt.process(name)\n",
        "    else:\n",
        "        #print('Failed to load a valid file')\n",
        "        #print('Returning an empty string')\n",
        "        document = ''\n",
        "    \n",
        "    #print(type(document))\n",
        "    return document\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2qFtc3ChyPo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **3. Function to tokenize the document**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0GX8MjUh4S9",
        "colab_type": "text"
      },
      "source": [
        "Input: String of text document\n",
        "\n",
        "Output: A list containing sentences as its elements\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4lbENC3ht3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the function used for tokenizing the sentences\n",
        "# tokenization of a sentence: '''provided in cell() above'''\n",
        "\n",
        "def tokenize(document):\n",
        "    # We are tokenizing using the PunktSentenceTokenizer\n",
        "    # we call an instance of this class as sentence_tokenizer\n",
        "    doc_tokenizer = PunktSentenceTokenizer()\n",
        "    \n",
        "    # tokenize() method: takes our document as input and returns a list of all the sentences in the document\n",
        "    \n",
        "    # sentences is a list containing each sentence of the document as an element\n",
        "    sentences_list = doc_tokenizer.tokenize(document)\n",
        "    return sentences_list\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnmuFXEfh-2i",
        "colab_type": "text"
      },
      "source": [
        "# **4. Read the document**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndhgIOUvh8W8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a08b6089-31a6-4c76-db91-016a89855b3f"
      },
      "source": [
        "while True:\n",
        "  print(\"\\n\\n\")\n",
        "  document = readDoc()\n",
        "  if document == '':\n",
        "    sys.exit(0)\n",
        "\n",
        "  print('\\nThe length of the file is:', end=' ')\n",
        "  x=len(document)\n",
        "  print(x)\n",
        "  print('\\n')\n",
        "\n",
        "  sentences_list = tokenize(document)\n",
        "  cv = CountVectorizer()\n",
        "  cv_matrix = cv.fit_transform(sentences_list)\n",
        "  normal_matrix = TfidfTransformer().fit_transform(cv_matrix)\n",
        "  res_graph = normal_matrix * normal_matrix.T\n",
        "  nx_graph = nx.from_scipy_sparse_matrix(res_graph)\n",
        "  #nx.draw_circular(nx_graph)\n",
        "  ranks = nx.pagerank(nx_graph)\n",
        "  sentence_array = sorted(((ranks[i], s) for i, s in enumerate(sentences_list)), reverse=True)\n",
        "  sentence_array = np.asarray(sentence_array)\n",
        "  rank_max = float(sentence_array[0][0])\n",
        "  rank_min = float(sentence_array[len(sentence_array) - 1][0])\n",
        "  temp_array = []\n",
        "  flag = 0\n",
        "  if rank_max - rank_min == 0:\n",
        "    temp_array.append(0)\n",
        "    flag = 1\n",
        "  if flag != 1:\n",
        "    for i in range(0, len(sentence_array)):\n",
        "        temp_array.append((float(sentence_array[i][0]) - rank_min) / (rank_max - rank_min))\n",
        "\n",
        "  #print(len(temp_array))\n",
        "  threshold = (sum(temp_array) / len(temp_array)) + 0.2\n",
        "  sentence_list = []\n",
        "  if len(temp_array) > 1:\n",
        "    for i in range(0, len(temp_array)):\n",
        "        if temp_array[i] > threshold:\n",
        "                sentence_list.append(sentence_array[i][1])\n",
        "  else:\n",
        "    sentence_list.append(sentence_array[0][1])\n",
        "  model = sentence_list\n",
        "  summary = \" \".join(str(x) for x in sentence_list)\n",
        "  print('\\n')\n",
        "  print('Summary for the given document \\n\\n')\n",
        "  print(summary)\n",
        "  print('\\n\\n')\n",
        "  y=len(summary)\n",
        "  print('\\nLength of the summary is ',y)\n",
        "  print('\\n')\n",
        " \n",
        "  f = open('result.txt', 'w')\n",
        "  f.write(summary)\n",
        "  print('\\nSummary written to result.txt file successfully\\n')\n",
        "  f.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Please input a file name: Article.docx\n",
            "You have asked for the document Article.docx\n",
            "\n",
            "The length of the file is: 8090\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary for the given document \n",
            "\n",
            "\n",
            "Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs\n",
            "\n",
            "\n",
            "\n",
            "Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, machine vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. In November 2012, Ciresan et al.’s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. This is similar to the response of a neuron in the visual cortex to a specific stimulus. The depth of the Convolution filter (the input channels) must be equal to the number channels (depth) of the input feature map. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters. Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Length of the summary is  2129\n",
            "\n",
            "\n",
            "\n",
            "Summary written to result.txt file successfully\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Please input a file name: Article.pdf\n",
            "You have asked for the document Article.pdf\n",
            "\n",
            "The length of the file is: 2078\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary for the given document \n",
            "\n",
            "\n",
            "Consequently, there exists a need for a mechanism to reduce the size \n",
            "of the text, this project aims to address  the problem with the growing amount of textual \n",
            "information in the \n",
            "world with text summarization. The importance of \n",
            "sentences is decided  consists of understanding \n",
            "the original text and re\n",
            "-\n",
            "telling it in fewer \n",
            "words. An abstractive \n",
            "summarization method Text Summarization is \n",
            "condensing the source text into a shorter \n",
            "version preserving its information content and overall meaning. It uses linguistic methods to examine and interpret the text and then to find the \n",
            "new concepts and expressions to best describe it by generating a new shorter text that \n",
            "conveys the most important informat\n",
            "ion from the original text document.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Length of the summary is  745\n",
            "\n",
            "\n",
            "\n",
            "Summary written to result.txt file successfully\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En1G91bhbk3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}